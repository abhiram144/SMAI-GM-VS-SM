{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brown-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import sys\n",
    "from graphkitlearn.graphkitlearn.gklearn.utils import graphfiles\n",
    "import networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gklearn.utils import *\n",
    "import os\n",
    "import random\n",
    "from gklearn.ged.env import GEDEnv\n",
    "import numpy as np\n",
    "from time import process_time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import *\n",
    "import datetime\n",
    "pathWeb = \"./Web/Web/data/\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "military-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(filename, childrentagName):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    dirname_dataset = os.path.dirname(filename)\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "    y = []\n",
    "    children = list([elem for elem in root.find(childrentagName).iter() if elem is not root.find(childrentagName)])\n",
    "    for graph in children:\n",
    "        mol_filename = graph.attrib['file']\n",
    "        mol_class = graph.attrib['class']\n",
    "        data.append(graphfiles.loadGXL(dirname_dataset + '/' + mol_filename))\n",
    "        y.append(mol_class)\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vertical-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphHelper:\n",
    "    def __init__(self, trainData, mProtoTypes):\n",
    "        self.trainData = trainData\n",
    "        self.InitializeGraphToVector(trainData, mProtoTypes)\n",
    "        self.mProtoTypes = mProtoTypes\n",
    "        \n",
    "    def GetDistanceBetweenGraphs(self,graph1, graph2):\n",
    "        ged_env = GEDEnv() # initailize GED environment.\n",
    "        ged_env.set_edit_cost('CONSTANT', # GED cost type.\n",
    "                            edit_cost_constants=[3, 3, 1, 3, 3, 1] # edit costs.\n",
    "                            )  \n",
    "        ged_env.add_nx_graph(graph1, '') # add graph1\n",
    "        ged_env.add_nx_graph(graph2, '') # add graph2\n",
    "        listID = ged_env.get_all_graph_ids() # get list IDs of graphs\n",
    "        ged_env.init(init_type='LAZY_WITHOUT_SHUFFLED_COPIES') # initialize GED environment.\n",
    "        options = {'initialization_method': 'RANDOM', # or 'NODE', etc.\n",
    "                'threads': 1 # parallel threads.\n",
    "                }\n",
    "        ged_env.set_method('BIPARTITE', # GED method.\n",
    "                        options # options for GED method.\n",
    "                        )\n",
    "        ged_env.init_method() # initialize GED method.\n",
    "        dis = -1\n",
    "        try:\n",
    "            ged_env.run_method(listID[0], listID[1]) # run.\n",
    "            dis = ged_env.get_upper_bound(listID[0], listID[1])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return dis\n",
    "\n",
    "    def MaxEditDistance(self, graphSets, nodes, addedIndices):\n",
    "        distanceVector = np.empty(shape=(len(graphSets ), len(nodes)))\n",
    "        for graphIndex,graph in enumerate(graphSets):\n",
    "            for nodeIndex,node in enumerate(nodes):\n",
    "                dis = self.GetDistanceBetweenGraphs(graph, node)\n",
    "                distanceVector[graphIndex][nodeIndex] = dis\n",
    "        maxValue = -1\n",
    "        maxIndex = -1\n",
    "        for graphIndex in range(len(graphSets)):\n",
    "            if(graphIndex not in addedIndices):\n",
    "                maxDistanceIndex = np.argmax(distanceVector[graphIndex])\n",
    "                if(distanceVector[graphIndex][maxDistanceIndex] > maxValue):\n",
    "                    maxValue = distanceVector[graphIndex][maxDistanceIndex]\n",
    "                    maxIndex = graphIndex\n",
    "\n",
    "        return maxIndex\n",
    "\n",
    "    def SelectSpanningPrototypes(self, graphData, mprototypes):\n",
    "        choiceIndex = random.randrange(len(graphData))\n",
    "        graphSelected = [graphData[choiceIndex]]\n",
    "        graphSelectedIndex = [choiceIndex]\n",
    "\n",
    "        for selectors in range(mprototypes - 1):\n",
    "            maxEditDistanceIndex = self.MaxEditDistance(graphData, graphSelected, graphSelectedIndex)\n",
    "            graphSelectedIndex.append(maxEditDistanceIndex)\n",
    "            graphSelected.append(graphData[maxEditDistanceIndex])\n",
    "        return graphSelectedIndex\n",
    "    \n",
    "    def GraphToVector(self, graphSet):\n",
    "        vectorMatrix = np.empty(shape= (len(graphSet), self.mProtoTypes))\n",
    "        for row, graph in enumerate(graphSet):\n",
    "            for col,prototypeIndex in enumerate(self.selectedProtoTypes):\n",
    "                vectorMatrix[row][col] = self.GetDistanceBetweenGraphs(graph, self.trainData[prototypeIndex])\n",
    "        return vectorMatrix\n",
    "    \n",
    "    def InitializeGraphToVector(self, graphSet, mprotoTypes):\n",
    "        self.selectedProtoTypes = self.SelectSpanningPrototypes(graphSet, mprotoTypes)\n",
    "        #return self.GraphToVector(graphSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "matched-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainWeb, y_train = LoadData(pathWeb +\"train.cxl\", \"fingerprints\")\n",
    "XvalidateWeb, y_validate = LoadData(pathWeb +\"valid.cxl\", \"fingerprints\")\n",
    "XtestWeb, y_test = LoadData(pathWeb +\"test.cxl\", \"fingerprints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "graph = GraphHelper(XtrainWeb, 10)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Prototype Selection at \", str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "trainVector = graph.GraphToVector(XtrainWeb)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Train Conversion at \", str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "testVector = graph.GraphToVector(XtestWeb)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Test at \", str(now))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "validationVector = graph.GraphToVector(XvalidateWeb)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Validation at \", str(now))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-assurance",
   "metadata": {},
   "source": [
    "## Saving the computed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TrainVectorWeb\", trainVector)\n",
    "np.save(\"TestVectorWeb\", testVector)\n",
    "np.save(\"validateVectorWeb\", validationVector)\n",
    "with open('GraphHelperObjWeb', 'wb') as config_dictionary_file:\n",
    "    pickle.dump(graph, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-france",
   "metadata": {},
   "source": [
    "## Loading from saved files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVector = np.load(\"TrainVectorWeb.npy\")\n",
    "testVector = np.load(\"TestVectorWeb.npy\")\n",
    "validationVector = np.load(\"validateVectorWeb.npy\")\n",
    "with open('GraphHelperObjWeb', 'rb') as config_dictionary_file:\n",
    "    graph = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def delta_fast(ck, cl, distances):\n",
    "    values = distances[np.where(ck)][:, np.where(cl)]\n",
    "    values = values[np.nonzero(values)]\n",
    "\n",
    "    return np.min(values)\n",
    "    \n",
    "def big_delta_fast(ci, distances):\n",
    "    values = distances[np.where(ci)][:, np.where(ci)]\n",
    "    #values = values[np.nonzero(values)]\n",
    "            \n",
    "    return np.max(values)\n",
    "\n",
    "def dunn_fast(points, labels):\n",
    "    \"\"\" Dunn index - FAST (using sklearn pairwise euclidean_distance function)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points : np.array\n",
    "        np.array([N, p]) of all points\n",
    "    labels: np.array\n",
    "        np.array([N]) labels of all points\n",
    "    \"\"\"\n",
    "    distances = euclidean_distances(points)\n",
    "    ks = np.sort(np.unique(labels))\n",
    "    \n",
    "    deltas = np.ones([len(ks), len(ks)])*1000000\n",
    "    big_deltas = np.zeros([len(ks), 1])\n",
    "    \n",
    "    l_range = list(range(0, len(ks)))\n",
    "    \n",
    "    for k in l_range:\n",
    "        for l in (l_range[0:k]+l_range[k+1:]):\n",
    "            deltas[k, l] = delta_fast((labels == ks[k]), (labels == ks[l]), distances)\n",
    "        \n",
    "        big_deltas[k] = big_delta_fast((labels == ks[k]), distances)\n",
    "\n",
    "    di = np.min(deltas)/np.max(big_deltas)\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=22, random_state=0).fit(trainVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import rand_score\n",
    "score = rand_score(y_train,kmeans.predict(trainVector))\n",
    "print('Accuracy of Train:{0:f}'.format(score))\n",
    "\n",
    "score = rand_score(y_validate,kmeans.predict(validationVector))\n",
    "print('Accuracy of Validation:{0:f}'.format(score))\n",
    "\n",
    "score = rand_score(y_test,kmeans.predict(testVector))\n",
    "print('Accuracy of Test:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphHelperGM:\n",
    "    def __init__(self, trainData):\n",
    "        self.trainData = trainData\n",
    "        \n",
    "    def GraphToVector(self,graphSet):\n",
    "        ged_env = GEDEnv() # initailize GED environment.\n",
    "        ged_env.set_edit_cost('CONSTANT', # GED cost type.\n",
    "                            edit_cost_constants=[3, 3, 1, 3, 3, 1] # edit costs.\n",
    "                            )  \n",
    "        for graph in graphSet:\n",
    "            ged_env.add_nx_graph(graph, '') # add graph1\n",
    "        for graph in self.trainData:\n",
    "            ged_env.add_nx_graph(graph, '') # add graph1\n",
    "        listID = ged_env.get_all_graph_ids() # get list IDs of graphs\n",
    "        ged_env.init(init_type='LAZY_WITHOUT_SHUFFLED_COPIES') # initialize GED environment.\n",
    "        options = {'initialization_method': 'RANDOM', # or 'NODE', etc.\n",
    "                'threads': 1 # parallel threads.\n",
    "                }\n",
    "        ged_env.set_method('BIPARTITE', # GED method.\n",
    "                        options # options for GED method.\n",
    "                        )\n",
    "        ged_env.init_method() # initialize GED method.\n",
    "        \n",
    "        \n",
    "        vectorMatrix = np.empty(shape= (len(graphSet), len(self.trainData)))\n",
    "        for row, graph in enumerate(graphSet):\n",
    "            for col  in range(len(self.trainData)):\n",
    "                ged_env.run_method(listID[row], listID[len(graphSet) + col]) # run.\n",
    "                dis = ged_env.get_upper_bound(listID[row], listID[len(graphSet) + col])\n",
    "                vectorMatrix[row][col] = dis\n",
    "        return vectorMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphGM = GraphHelperGM(XtrainWeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "trainVectorGM = graphGM.GraphToVector(XtrainWeb)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Test at \", str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "testVectorGM = graphGM.GraphToVector(XtestWeb)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Test at \", str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = process_time() \n",
    "validationVectorGM = graphGM.GraphToVector(XvalidateWeb)\n",
    "t1_stop = process_time()\n",
    "print(t1_stop - t1_start)\n",
    "now = datetime.datetime.now()\n",
    "print(\"Completed Test at \", str(now))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-partition",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TrainVectorWebGM\", trainVectorGM)\n",
    "np.save(\"TestVectorWebGM\", testVectorGM)\n",
    "np.save(\"validateVectorWebGM\", validationVectorGM)\n",
    "with open('GraphHelperObjWebGM', 'wb') as config_dictionary_file:\n",
    "    pickle.dump(graphGM, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-workshop",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVectorGM = np.load(\"TrainVectorWebGM.npy\")\n",
    "testVectorGM = np.load(\"TestVectorWebGM.npy\")\n",
    "validationVectorGM = np.load(\"validateVectorWebGM.npy\")\n",
    "with open('GraphHelperObjWebGM', 'rb') as config_dictionary_file:\n",
    "    graphGM = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import rand_score\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "\n",
    "kmeansGM = KMeans(n_clusters=22, random_state=0).fit(trainVectorGM)\n",
    "pred = kmeansGM.predict(trainVectorGM)\n",
    "score = rand_score(y_train,pred)\n",
    "print('Rand Index Accuracy of Train:{0:f}'.format(score))\n",
    "print(\"Homogenity Score of Train %.6f\" % homogeneity_score(y_train, pred))\n",
    "print('Dunn Index of the Cluster is :{0:f}'.format(dunn_fast(trainVectorGM, pred)))\n",
    "le = LabelEncoder()\n",
    "y_train_labels = le.fit_transform(y_train)\n",
    "print('Dunn Index of the Ground Truth is :{0:f}'.format(dunn_fast(trainVectorGM, y_train_labels)))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "pred = kmeansGM.predict(validationVectorGM)\n",
    "score = rand_score(y_validate,pred)\n",
    "print('Rand Index Accuracy of Validation:{0:f}'.format(score))\n",
    "print(\"Homogenity Score of Validation %.6f\" % homogeneity_score(y_validate, pred))\n",
    "print('Dunn Index of the Cluster is :{0:f}'.format(dunn_fast(validationVectorGM, pred)))\n",
    "le = LabelEncoder()\n",
    "y_validate_labels = le.fit_transform(y_validate)\n",
    "print('Dunn Index of the Ground Truth is :{0:f}'.format(dunn_fast(validationVectorGM, y_validate_labels)))\n",
    "\n",
    "print()\n",
    "print()\n",
    "pred = kmeansGM.predict(testVectorGM)\n",
    "score = rand_score(y_test,pred)\n",
    "print('Rand Index Accuracy of Test:{0:f}'.format(score))\n",
    "print(\"Homogenity Score of Test %.6f\" % homogeneity_score(y_test, pred))\n",
    "print('Dunn Index of the Cluster is :{0:f}'.format(dunn_fast(testVectorGM, pred)))\n",
    "le = LabelEncoder()\n",
    "y_train_labels = le.fit_transform(y_test)\n",
    "print('Dunn Index of the Ground Truth is :{0:f}'.format(dunn_fast(testVectorGM, y_train_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
